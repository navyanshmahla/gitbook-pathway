# High level LLM Architecture Components for In-context Learning

In this brief video, Anup provides a high-level breakdown of how in-context learning operates within an LLM. He'll guide you through the journey of a prompt as it interacts with a vector database or index, undergoes similarity search, feeds context, and eventually results in a coherent LLM output.

{% embed url="https://www.youtube.com/embed/OXZQBXBvOR4?start=595&end=703" %}

Understanding this architecture is essential for mastering the interaction between prompts and LLMs, a crucial skill for anyone looking to effectively deploy these models in a variety of settings.

As you may have noticed, the concepts explained under 'in-context learning' in the forthcoming sections are essentially what RAG accomplishes. In-context learning allows your LLM to adapt and respond based on not just the pre-trained data, but also from the external, real-time information it retrieves. This is precisely what RAG is designed to do.
