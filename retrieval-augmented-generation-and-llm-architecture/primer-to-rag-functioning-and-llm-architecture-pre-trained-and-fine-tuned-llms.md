# Primer to RAG Functioning and LLM Architecture: Pre-trained and Fine-tuned LLMs

Welcome back to our module on LLM Architecture and RAG!&#x20;

Up next is a series of learning resources created by Anup Surendran that sets the stage for your journey ahead. This video serves as a primer, acquainting you with key concepts such as pre-training, RLHF (Reinforcement Learning from Human Feedback), fine-tuning, and in-context learning.

{% embed url="https://www.youtube.com/embed/OXZQBXBvOR4?end=283&start=0" %}

These aren't just buzzwords; they're your toolkit for unlocking the full potential of Large Language Models. Understanding these terms will be crucial as they lay the groundwork for our upcoming module, which delves into 'In-Context Learning.' So, stay tuned!

**Resources for Later:** Once you've completed this module on RAG and LLM Architecture, you can explore these resources for a more comprehensive understanding of RLHF.&#x20;

* For a beginner-friendly introduction, check out the [video by HuggingFace](https://youtu.be/2MBJOuVq380), which offers an accessible explanation of RLHF concepts.&#x20;
* Additionally, for a slightly deeper dive focusing on the mathematical aspects, consider the [lecture on RLHF](https://youtu.be/SXpJ9EmG3s4?t=2125) available by Stanford Online on their YouTube channel.&#x20;

\
