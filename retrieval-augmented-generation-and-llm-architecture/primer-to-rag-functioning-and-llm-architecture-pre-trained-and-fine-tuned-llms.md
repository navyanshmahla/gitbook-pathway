# Primer to RAG Functioning and LLM Architecture: Pre-trained and Fine-tuned LLMs

Welcome back to our module on LLM Architecture and RAG!&#x20;

Up next is a series of learning resources created by Anup Surendran that sets the stage for your journey ahead. This video serves as a primer, acquainting you with key concepts such as pre-training, RLHF (Reinforcement Learning from Human Feedback), fine-tuning, and in-context learning.

{% embed url="https://www.youtube.com/embed/OXZQBXBvOR4?end=283&start=0" %}

These aren't just buzzwords; they're your toolkit for unlocking the full potential of Large Language Models. Understanding these terms will be crucial as they lay the groundwork for our upcoming module, which delves into 'In-Context Learning.' So, stay tuned!

\
